{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 2.7.15 |Anaconda, Inc.| (default, May  1 2018, 18:37:09) [MSC v.1500 64 bit (AMD64)]\n",
      "Nltk: 3.3\n",
      "sklearn: 0.19.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "\n",
    "print('Python: {}'.format(sys.version))\n",
    "print('Nltk: {}'.format(nltk.__version__))\n",
    "print('sklearn: {}'.format(sklearn.__version__))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### corpora\n",
    "corpora :plural of body of texts\n",
    "### lexicon\n",
    "dictionary word with their meanings\n",
    "### tokenized\n",
    "Each entity that is a part of whatever was split up based on rules. For examples . each word is a token when a sentence is tokenized into words. each sentence can also be token if you tokenize the sentence out of paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Students , ook great today.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "text = \"Hello Students , ook great today.\"\n",
    "print(sent_tokenize(text))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello , baby boy there is some scope left.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(\"hello , baby boy there is some scope left.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Students', ',', 'ook', 'great', 'today', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stop words , i.e useless data\n",
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words('english')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'some', 'sample', 'text', ',', 'showing', 'off', 'the', 'stop', 'words', 'filteration']\n",
      "['sample', 'text', ',', 'showing', 'stop', 'words', 'filteration']\n",
      "['sample', 'text', ',', 'showing', 'stop', 'words', 'filteration']\n"
     ]
    }
   ],
   "source": [
    "example = \"this is some sample text , showing off the stop words filteration\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "    \n",
    "print(word_tokens)\n",
    "print(sentence)\n",
    "\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#stemming words with NLTK\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = {'rider','riding','ride','rides'}\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when\n",
      "rider\n",
      "are\n",
      "ride\n",
      "their\n",
      "hors\n",
      ",\n",
      "they\n",
      "often\n",
      "think\n",
      "of\n",
      "how\n",
      "cowboy\n",
      "rode\n",
      "hors\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#stemming an entire distance\n",
    "new_text = \"When riders are riding their horses , they often think of how cowboys rode horses.\"\n",
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import udhr\n",
    "print(udhr.raw('English-Latin1'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have some test now we can train the PunktSentenceTokenizer\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets use this to tokenize the sample text\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function which will tag each word with a part of speech\n",
    "def process_content(): \n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "    except exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "        \n",
    "process_content()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.help.upenn_tagset()\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-8c76a1474e1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mprocess_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-8c76a1474e1f>\u001b[0m in \u001b[0;36mprocess_content\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mchunked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunkParser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mchunked\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[1;31m#draw the chunks with nltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\GUNISH\\Anaconda2\\lib\\site-packages\\nltk\\tree.pyc\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    688\u001b[0m         \"\"\"\n\u001b[0;32m    689\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdraw_trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m         \u001b[0mdraw_trees\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhighlight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\GUNISH\\Anaconda2\\lib\\site-packages\\nltk\\draw\\tree.pyc\u001b[0m in \u001b[0;36mdraw_trees\u001b[1;34m(*trees)\u001b[0m\n\u001b[0;32m    861\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m     \"\"\"\n\u001b[1;32m--> 863\u001b[1;33m     \u001b[0mTreeView\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    864\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\GUNISH\\Anaconda2\\lib\\site-packages\\nltk\\draw\\tree.pyc\u001b[0m in \u001b[0;36mmainloop\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    852\u001b[0m         \"\"\"\n\u001b[0;32m    853\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0min_idle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 854\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdraw_trees\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\GUNISH\\Anaconda2\\lib\\lib-tk\\Tkinter.pyc\u001b[0m in \u001b[0;36mmainloop\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1127\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m         \u001b[1;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[1;34m\"\"\"Quit the Tcl interpreter. All widgets will be destroyed.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#chunking with NLTK\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "#Now that we have some text we can train PunktSentenceTokenizer\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "#Now let's tokenize the sample text\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content(): \n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            #combine the part of speech tag wiith the regular expression\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?} \"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            chunked.draw()\n",
    "            #draw the chunks with nltk\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "        \n",
    "process_content()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chinking with NLTK\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "#Now that we have some text we can train PunktSentenceTokenizer\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "#Now let's tokenize the sample text\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content(): \n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            #combine the part of speech tag wiith the regular expression\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>*} \n",
    "                                         }<VB.?|IN|DT|TO>*{\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            chunked.draw()\n",
    "            #draw the chunks with nltk\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Named Entity Recognition\n",
    "\n",
    "def process_content(): \n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            namedEnt = nltk.ne_chunk(tagged,binary= False)\n",
    "            \n",
    "            for subtree in chunked.subtrees(filter = lambda t: t.label() == 'Chunk'):\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            namedEnt.draw()\n",
    "            #draw the chunks with nltk\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously Covered\n",
    "-> Tokenizing\n",
    "-> stemming\n",
    "->part-of-speech-tagging\n",
    "->chunking chinking\n",
    "-> named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Text classification to label Movie review as positive and Negative review using Support Vector Machine\n",
    "import random\n",
    "import nltk \n",
    "from nltk.corpus import movie_reviews\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of Documents: {}', '2000')\n",
      "('first Review: {}', '([u\\'overblown\\', u\\'remake\\', u\\'of\\', u\\'the\\', u\\'1963\\', u\\'robert\\', u\\'wise\\', u\\'film\\', u\\'of\\', u\\'the\\', u\\'same\\', u\\'name\\', u\\'(\\', u\\'based\\', u\\'on\\', u\\'shirley\\', u\\'jackson\\', u\"\\'\", u\\'s\\', u\\'novel\\', u\\'\"\\', u\\'the\\', u\\'haunting\\', u\\'of\\', u\\'hill\\', u\\'house\\', u\\'\"\\', u\\')\\', u\\'stars\\', u\\'lili\\', u\\'taylor\\', u\\'as\\', u\\'one\\', u\\'of\\', u\\'three\\', u\\'\"\\', u\\'lab\\', u\\'rats\\', u\\'\"\\', u\\'that\\', u\\'participate\\', u\\'in\\', u\\'a\\', u\\'supposed\\', u\\'\"\\', u\\'insomnia\\', u\\'\"\\', u\\'study\\', u\\'being\\', u\\'initiated\\', u\\'by\\', u\\'liam\\', u\\'neeson\\', u\\'.\\', u\\'he\\', u\\'is\\', u\\'actually\\', u\\'conducting\\', u\\'a\\', u\\'study\\', u\\'on\\', u\\'the\\', u\\'causes\\', u\\'of\\', u\\'fear\\', u\\'and\\', u\\'why\\', u\\'the\\', u\\'human\\', u\\'mind\\', u\\'is\\', u\\'still\\', u\\'affected\\', u\\'by\\', u\\'it\\', u\\'.\\', u\\'he\\', u\\'drags\\', u\\'the\\', u\\'test\\', u\\'subjects\\', u\\'out\\', u\\'to\\', u\\'a\\', u\\'foreboding\\', u\\'mansion\\', u\\'where\\', u\\'all\\', u\\'types\\', u\\'of\\', u\\'\"\\', u\\'creepy\\', u\\'\"\\', u\\'cgi\\', u\\'effects\\', u\\'scare\\', u\\'the\\', u\\'cast\\', u\\'and\\', u\\'try\\', u\\'the\\', u\\'audience\\', u\"\\'\", u\\'s\\', u\\'patience\\', u\\'.\\', u\\'i\\', u\\'know\\', u\\'this\\', u\\'question\\', u\\'has\\', u\\'been\\', u\\'posed\\', u\\'before\\', u\\',\\', u\\'but\\', u\\'why\\', u\\'do\\', u\\'people\\', u\\'keep\\', u\\'remaking\\', u\\'good\\', u\\'movies\\', u\\'?\\', u\\'the\\', u\\'original\\', u\\'was\\', u\\'a\\', u\\'great\\', u\\'psychological\\', u\\'horror\\', u\\'film\\', u\\'.\\', u\\'this\\', u\\'new\\', u\\'version\\', u\\'is\\', u\\'dull\\', u\\'and\\', u\\'very\\', u\\'decidedly\\', u\\'not\\', u\\'scary\\', u\\'.\\', u\\'in\\', u\\'fact\\', u\\',\\', u\\'i\\', u\\'feel\\', u\\'it\\', u\\'is\\', u\\'downright\\', u\\'impossible\\', u\\'to\\', u\\'make\\', u\\'a\\', u\\'modern\\', u\\'day\\', u\\'pg\\', u\\'-\\', u\\'13\\', u\\'horror\\', u\\'movie\\', u\\'that\\', u\\'is\\', u\\'scary\\', u\\'.\\', u\\'unless\\', u\\'you\\', u\\'find\\', u\\'obvious\\', u\\'computer\\', u\\'generated\\', u\\'special\\', u\\'effects\\', u\\'frightening\\', u\\',\\', u\\'then\\', u\\'there\\', u\"\\'\", u\\'s\\', u\\'nothing\\', u\\'in\\', u\\'this\\', u\\'film\\', u\\'that\\', u\\'will\\', u\\'raise\\', u\\'hackles\\', u\\'.\\', u\\'i\\', u\\'nearly\\', u\\'fell\\', u\\'asleep\\', u\\'twice\\', u\\'during\\', u\\'the\\', u\\'film\\', u\\',\\', u\\'and\\', u\\'probably\\', u\\'would\\', u\\'have\\', u\\'if\\', u\\'they\\', u\\'guy\\', u\\'two\\', u\\'rows\\', u\\'behind\\', u\\'me\\', u\\'didn\\', u\"\\'\", u\\'t\\', u\\'seem\\', u\\'to\\', u\\'have\\', u\\'such\\', u\\'a\\', u\\'personal\\', u\\'relationship\\', u\\'with\\', u\\'the\\', u\\'characters\\', u\\'that\\', u\\'he\\', u\\'felt\\', u\\'he\\', u\\'needed\\', u\\'to\\', u\\'discuss\\', u\\'every\\', u\\'decision\\', u\\'they\\', u\\'made\\', u\\'with\\', u\\'them\\', u\\'as\\', u\\'the\\', u\\'film\\', u\\'progressed\\', u\\'.\\', u\\'it\\', u\"\\'\", u\\'s\\', u\\'sad\\', u\\'to\\', u\\'see\\', u\\'such\\', u\\'an\\', u\\'amazing\\', u\\'cast\\', u\\'wasted\\', u\\'so\\', u\\'badly\\', u\\'.\\', u\\'owen\\', u\\'wilson\\', u\\'spends\\', u\\'most\\', u\\'of\\', u\\'the\\', u\\'film\\', u\\'wandering\\', u\\'the\\', u\\'halls\\', u\\'of\\', u\\'the\\', u\\'house\\', u\\',\\', u\\'and\\', u\\'the\\', u\\'script\\', u\\'does\\', u\\'no\\', u\\'justice\\', u\\'to\\', u\\'his\\', u\\'wonderful\\', u\\'comic\\', u\\'ability\\', u\\'.\\', u\\'catherine\\', u\\'zeta\\', u\\'-\\', u\\'jones\\', u\\'(\\', u\\'always\\', u\\'nice\\', u\\'to\\', u\\'look\\', u\\'at\\', u\\')\\', u\\'is\\', u\\'given\\', u\\'the\\', u\\'woefully\\', u\\'underwritten\\', u\\'role\\', u\\'of\\', u\\'a\\', u\\'bi\\', u\\'-\\', u\\'sexual\\', u\\'insomniac\\', u\\'that\\', u\\'gets\\', u\\'to\\', u\\'run\\', u\\'out\\', u\\'of\\', u\\'her\\', u\\'bedroom\\', u\\'perplexed\\', u\\'every\\', u\\'time\\', u\\'some\\', u\\'strange\\', u\\'noise\\', u\\'occurs\\', u\\'.\\', u\\'liam\\', u\\'neeson\\', u\\'pops\\', u\\'in\\', u\\'from\\', u\\'time\\', u\\'to\\', u\\'time\\', u\\'to\\', u\\'talk\\', u\\'into\\', u\\'his\\', u\\'tape\\', u\\'recorder\\', u\\'and\\', u\\'attempt\\', u\\'to\\', u\\'convince\\', u\\'the\\', u\\'others\\', u\\'that\\', u\\'he\\', u\\'has\\', u\\'as\\', u\\'much\\', u\\'knowledge\\', u\\'about\\', u\\'what\\', u\"\\'\", u\\'s\\', u\\'going\\', u\\'on\\', u\\'as\\', u\\'the\\', u\\'rest\\', u\\'of\\', u\\'them\\', u\\'do\\', u\\'.\\', u\\'finally\\', u\\',\\', u\\'poor\\', u\\'lili\\', u\\'taylor\\', u\\',\\', u\\'the\\', u\\'center\\', u\\'of\\', u\\'the\\', u\\'film\\', u\\',\\', u\\'gets\\', u\\'the\\', u\\'brunt\\', u\\'of\\', u\\'the\\', u\\'cgi\\', u\\'effects\\', u\\'thrown\\', u\\'at\\', u\\'her\\', u\\'while\\', u\\'everyone\\', u\\'else\\', u\\'gets\\', u\\'to\\', u\\'scream\\', u\\'and\\', u\\'try\\', u\\'to\\', u\\'rescue\\', u\\'her\\', u\\'.\\', u\\'apparently\\', u\\',\\', u\\'a\\', u\\'subplot\\', u\\'involving\\', u\\'a\\', u\\'tryst\\', u\\'between\\', u\\'zeta\\', u\\'-\\', u\\'jones\\', u\\'and\\', u\\'taylor\\', u\"\\'\", u\\'s\\', u\\'characters\\', u\\'was\\', u\\'filmed\\', u\\'but\\', u\\'removed\\', u\\'.\\', u\\'that\\', u\"\\'\", u\\'s\\', u\\'too\\', u\\'bad\\', u\\',\\', u\\'because\\', u\\'it\\', u\\'might\\', u\\'have\\', u\\'lent\\', u\\'some\\', u\\'better\\', u\\'characterization\\', u\\'to\\', u\\'the\\', u\\'narrative\\', u\\'.\\', u\\'this\\', u\\'is\\', u\\'jan\\', u\\'de\\', u\\'bont\\', u\"\\'\", u\\'s\\', u\\'second\\', u\\'straight\\', u\\'misfire\\', u\\'(\\', u\\'speed\\', u\\'2\\', u\\':\\', u\\'cruise\\', u\\'control\\', u\\'being\\', u\\'the\\', u\\'first\\', u\\')\\', u\\'.\\', u\\'when\\', u\\'is\\', u\\'he\\', u\\'going\\', u\\'to\\', u\\'learn\\', u\\'that\\', u\\'bigger\\', u\\'is\\', u\\'not\\', u\\'always\\', u\\'necessarily\\', u\\'better\\', u\\'?\\', u\\'robert\\', u\\'wise\\', u\\'knew\\', u\\'that\\', u\\'when\\', u\\'he\\', u\\'made\\', u\\'the\\', u\\'original\\', u\\'and\\', u\\'the\\', u\\'makes\\', u\\'of\\', u\\'the\\', u\\'blair\\', u\\'witch\\', u\\'project\\', u\\'also\\', u\\'knew\\', u\\'that\\', u\\'.\\', u\\'one\\', u\\'should\\', u\\'not\\', u\\'approach\\', u\\'a\\', u\\'haunted\\', u\\'house\\', u\\'movie\\', u\\'with\\', u\\'a\\', u\\'twister\\', u\\'mindset\\', u\\'.\\', u\\'if\\', u\\'de\\', u\\'bont\\', u\\'and\\', u\\'screenwriter\\', u\\'david\\', u\\'self\\', u\\'had\\', u\\'let\\', u\\'our\\', u\\'minds\\', u\\'fill\\', u\\'in\\', u\\'the\\', u\\'blanks\\', u\\'as\\', u\\'to\\', u\\'what\\', u\\'was\\', u\\'happening\\', u\\'instead\\', u\\'of\\', u\\'showing\\', u\\'us\\', u\\'everything\\', u\\',\\', u\\'it\\', u\\'would\\', u\\'have\\', u\\'served\\', u\\'to\\', u\\'make\\', u\\'the\\', u\\'film\\', u\\'terrifying\\', u\\'.\\', u\\'what\\', u\\'our\\', u\\'mind\\', u\\'fashions\\', u\\'on\\', u\\'it\\', u\"\\'\", u\\'s\\', u\\'own\\', u\\'can\\', u\\'be\\', u\\'a\\', u\\'thousand\\', u\\'times\\', u\\'more\\', u\\'frightening\\', u\\'than\\', u\\'having\\', u\\'having\\', u\\'it\\', u\\'completely\\', u\\'led\\', u\\'towards\\', u\\'everything\\', u\\'.\\', u\\'apparently\\', u\\',\\', u\\'yet\\', u\\'another\\', u\\'version\\', u\\'of\\', u\\'this\\', u\\'story\\', u\\'is\\', u\\'being\\', u\\'filmed\\', u\\'under\\', u\\'it\\', u\"\\'\", u\\'s\\', u\\'original\\', u\\'title\\', u\\'.\\', u\\'i\\', u\\'certainly\\', u\\'hope\\', u\\'that\\', u\\'this\\', u\\'newer\\', u\\'rendition\\', u\\'will\\', u\\'not\\', u\\'be\\', u\\'hurt\\', u\\'by\\', u\\'the\\', u\\'insanity\\', u\\'of\\', u\\'jan\\', u\\'de\\', u\\'bont\\', u\"\\'\", u\\'s\\', u\\'version\\', u\\'and\\', u\\'that\\', u\\'it\\', u\\'will\\', u\\'retain\\', u\\'the\\', u\\'psychological\\', u\\'impact\\', u\\'that\\', u\\'the\\', u\\'original\\', u\\'release\\', u\\'had\\', u\\'.\\', u\\'[\\', u\\'pg\\', u\\'-\\', u\\'13\\', u\\']\\'], u\\'neg\\')')\n",
      "Most common words:[(u',', 77717), (u'the', 76529), (u'.', 65876), (u'a', 38106), (u'and', 35576), (u'of', 34123), (u'to', 31937), (u\"'\", 30585), (u'is', 25195), (u'in', 21822), (u's', 18513), (u'\"', 17612), (u'it', 16107), (u'that', 15924), (u'-', 15595)]\n",
      "The word happy: 215\n"
     ]
    }
   ],
   "source": [
    "#build a list of documents\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "            for category in movie_reviews.categories()\n",
    "            for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "#shuffle the documents\n",
    "random.shuffle(documents)\n",
    "\n",
    "print('Number of Documents: {}', format(len(documents)))\n",
    "print('first Review: {}',format(documents[0]))\n",
    "\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "print('Most common words:{}'.format(all_words.most_common(15))) \n",
    "print('The word happy: {}'.format(all_words[\"happy\"]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39768\n"
     ]
    }
   ],
   "source": [
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use the 4000 most common words as features\n",
    "word_features = list(all_words.keys())[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shows\n",
      "kids\n",
      "plot\n",
      "music\n",
      "want\n",
      "production\n",
      "feeling\n",
      "away\n",
      ".\n",
      "has\n",
      "confusing\n",
      "bottom\n",
      "exact\n",
      "years\n",
      "still\n",
      "now\n",
      "didn\n",
      "one\n",
      "s\n",
      "world\n",
      "arrow\n",
      "with\n",
      "concept\n",
      "7\n",
      "horror\n",
      "more\n",
      "visions\n",
      "american\n",
      "feels\n",
      "also\n",
      "into\n",
      "video\n",
      "makes\n",
      "start\n",
      "tons\n",
      "despite\n",
      "meantime\n",
      "'\n",
      "insight\n",
      "off\n",
      "not\n",
      "wes\n",
      "problem\n"
     ]
    }
   ],
   "source": [
    "#Build a find features function that will determine which of the 4000 word features contained in a review\n",
    "\n",
    "def find_features(document):\n",
    "    words=set(document)\n",
    "    features={}\n",
    "    \n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "        \n",
    "    return features\n",
    "\n",
    "#lets use an example from negative review\n",
    "\n",
    "features = find_features(movie_reviews.words('neg/cv000_29416.txt'))\n",
    "\n",
    "for key,value in features.items():\n",
    "    if value == True:\n",
    "        print key\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets do it for all the example\n",
    "\n",
    "featuresets = [(find_features(rev),category) for (rev,category ) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can split the featureset into training and testing dataset using sklearn\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "#define a seed for repoducability\n",
    "seed =1\n",
    "\n",
    "#splitting the data into training and testing datasets\n",
    "\n",
    "training, testing = model_selection.train_test_split(featuresets,test_size=0.25,random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(training))\n",
    "print(len(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how we use sklearn algorithm in NLTK\n",
    "\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= SklearnClassifier(SVC(kernel='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model on the training data\n",
    "model.train(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC accuracy: 0.66\n"
     ]
    }
   ],
   "source": [
    "#test on the testing dataset:\n",
    "accuracy = nltk.classify.accuracy(model,testing)\n",
    "print('SVC accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
